#!/usr/bin/env python

### importing packages
import argparse
import os
import numpy as np
import pandas as pd
import pickle
import datetime
import tsinfer
import tsdate
import math
import msprime
import tskit

from scipy.spatial import distance_matrix
from egrm import varGRM_C, mTMRCA_C


### ad-hoc functions ###
def step_mig_mat(m, nrow, ncol):
  pmat = np.arange(0, nrow * ncol).reshape(nrow, ncol)
  mmat = np.zeros(shape = [nrow * ncol, nrow * ncol])
  
  def contain(ix,max_ix):
    if ix<0:
      return(0)
    if ix>(max_ix-1):
      return(max_ix-1)
    else:
      return(ix)
  
  for ii in range(nrow * ncol):
    center_ix=np.where(pmat==ii)
    top_ix=pmat[contain(center_ix[0]-1,nrow),contain(center_ix[1],ncol)]
    bottom_ix=pmat[contain(center_ix[0]+1,nrow),contain(center_ix[1],ncol)]
    left_ix=pmat[contain(center_ix[0],nrow),contain(center_ix[1]-1,ncol)]
    right_ix=pmat[contain(center_ix[0],nrow),contain(center_ix[1]+1,ncol)]
    
    mmat[ii,top_ix]=mmat[ii,bottom_ix]=mmat[ii,left_ix]=mmat[ii,right_ix]=m
    mmat[top_ix,ii]=mmat[bottom_ix,ii]=mmat[left_ix,ii]=mmat[right_ix,ii]=m
    mmat[ii,ii]=0
  
  return(mmat)

def Fst(trees, ns):
  ns = np.array(ns)
  labels = np.repeat(np.arange(ns.shape[0]), ns)
  buffer = []
  for v in trees.variants():
    genotypes = v.genotypes
    p = genotypes.mean()
    q = p * (1-p)
    ps = np.array([genotypes[labels == i].mean() for i in range(ns.shape[0])])
    qs = ps * (1 - ps) * ns / ns.sum()
    fst = (q - qs.sum()) / q
    buffer.append(fst)
  return np.array(buffer).mean()

def Fst2(trees, study_ids, reference_ids):
  n1 = study_ids.shape[0]
  n2 = reference_ids.shape[0]
  
  buffer = []
  for v in trees.variants():
    genotypes = v.genotypes
    p = genotypes[np.union1d(study_ids, reference_ids)].mean(); q = p * (1-p)
    if p == 0 or p == 1: continue
    p1 = genotypes[study_ids].mean(); q1 = p1 * (1-p1)
    p2 = genotypes[reference_ids].mean(); q2 = p2 * (1-p2)
    fst = (q - (q1*n1 + q2*n2) / (n1 + n2)) / q
    buffer.append(fst)
  
  return np.array(buffer).mean()

def remove_monomorphic(trees):
  tables = trees.tables
  tables.sites.clear()
  tables.mutations.clear()
  n = trees.num_samples
  for tree in trees.trees():
     for site in tree.sites():
        visited = False
        for mutation in site.mutations:
           k = tree.num_samples(mutation.node)
           if k > 0 and k < n:
               if not visited:
                   visited = True
                   site_id = tables.sites.add_row(site.position, site.ancestral_state, metadata=site.metadata)
               tables.mutations.add_row(site_id, mutation.node, mutation.derived_state, parent=-1, metadata=None)
  tables.compute_mutation_parents()
  return tables.tree_sequence()

def getX(trees, idx):
  N = trees.num_samples
  M = idx.shape[0]
  X = np.zeros((N, M)).astype("int")
  i = 0; num = 0
  for v in trees.variants():
    if i in idx:
      X[:, num] = v.genotypes; num += 1
    i += 1
  return X

def getK(trees, idx):
  N = trees.num_samples
  M = idx.shape[0]
  K_all = np.zeros((N, N))
  for idx_ in np.split(idx, range(1000, M, 1000)):
    Z = getX(trees, idx_).astype("float")
    Z -= Z.mean(axis=0); Z /= Z.std(axis=0)
    K_all += np.dot(Z, np.transpose(Z))
    del Z
  K_all /= M
  return K_all

def separation_index(true_labels, x, dmat = None):
  if dmat is None: 
    dmat = distance_matrix(x, x)
  omat = dmat.argsort(axis=1)
  
  scores = []
  for i in np.arange(true_labels.shape[0]):
    fellows = np.where(true_labels == true_labels[i])[0]
    neighbors = omat[i, :fellows.shape[0]]
    intersects = np.intersect1d(neighbors, fellows)
    score = intersects.shape[0] / fellows.shape[0]
    scores.append(score)
  
  scores = np.array(scores)
  return(scores)


### parse arguments ###
parser=argparse.ArgumentParser(description="""some help texts.""",
                               epilog="""some help texts.""")

# name of simulation
parser.add_argument('--name', default = 'tmp', type = str, help='output file name')

# workflow control
parser.add_argument('--run_all', '--all', action='store_true', help='run all steps')
parser.add_argument('--run_egrm', '--egrm', action='store_true', help='run egrm on the true tree sequence')
parser.add_argument('--run_relate', '--relate', action='store_true', help='run relate tree reconstruction')
parser.add_argument('--run_tsinfer', '--tsinfer', action='store_true', help='run tsinfer tree reconstruction')
parser.add_argument('--run_mtmrca', '--mtmrca', action='store_true', help='compute mean TMRCA')
parser.add_argument('--run_impute', '--impute', action='store_true', help='compute K_imputed')

# demography and population structure
parser.add_argument('--demo', type = str, default = 'ooa', help='demography model')
parser.add_argument('--admixture', action='store_true', help='simulate admixture, otherwise split')
parser.add_argument('--nrow', type = int, default = 1, help='number of rows of populations')
parser.add_argument('--ncol', type = int, default = 1, help='number of columns of populations')
parser.add_argument('--migration_rate', type = float, default = 0.01, help='migration rate')
parser.add_argument('--time_move', type = float, default = 100, help='move time')
parser.add_argument('--pop_size', type = int, default = 1000, help='population size (only applies when using constant demo)')

# sample sizes
parser.add_argument('--N', type = int, help='total sample size')
parser.add_argument('--n', type = int, help='sample size in each population')
parser.add_argument('--ns', type = int, nargs='*', help='list of sample sizes in each population')
parser.add_argument('--ns_ref', type = int, nargs='*', help='list of sample sizes in each population')

# genetics
parser.add_argument('--l', type = int, default = 3e7, help='chromosome length')
parser.add_argument('--mutation_rate', default = 1e-8, type = float, help='mutation rate')
parser.add_argument('--recomb_rate', default = 1e-8, type = float, help='recombination rate')
parser.add_argument('--cas_ratio', default = 0.1, type = float, help='M_cas / M')
parser.add_argument('--obs_ratio', default = 0.2, type = float, help='M_obs / M_5')
parser.add_argument('--h2g', default = 1.0, type = float, help='heritability')
parser.add_argument('--Alpha', default = -1, type = float, help='causal probability parameter')
parser.add_argument('--Beta', default = 1, type = float, help='observation probability parameter')

args = vars(parser.parse_args())
locals().update(args)

name = args.pop("name")
os.system("rm -r -f " + name + " && mkdir " + name)
os.chdir(name)

if run_all:
  run_egrm = True
  run_relate = True
  run_tsinfer = True
  run_mtmrca = True
  run_impute = True


### processing samples sizes ###
if (N is not None) + (n is not None) + (ns is not None) != 1:
  print("one among N, n and ns should be specified")
  sys.exit()

if (N is not None):
  if N % (nrow * ncol) != 0:
    print("only one among N, n and ns should be specified")
    sys.exit()
  ns = [int(N / nrow / ncol)] * nrow * ncol

if (n is not None):
  N = n * nrow * ncol
  ns = [n] * nrow * ncol

if (ns is not None):
  if len(ns) != nrow * ncol:
    print("length of ns should equal nrow * ncol")
    sys.exit()
  N = sum(ns)

if (ns_ref is not None):
  if len(ns_ref) != nrow * ncol:
    print("length of ns_ref should equal nrow * ncol")
    sys.exit()

if (ns_ref is None):
  ns_ref = [0] * nrow * ncol

N_ref = sum(ns_ref)

del n

### simulate genotypes ###
print("simulating genotypes")

N_B = 2100
N_EU0 = 1000
N_AF = 12300
N_A = 7300
r_EU = 0.004
generation_time = 25
T_EU_AS = 21.2e3 / generation_time
T_B = 140e3 / generation_time
T_AF = 220e3 / generation_time
N_EU = N_EU0 / math.exp(-r_EU * T_EU_AS)

migration_matrix = step_mig_mat(migration_rate, nrow, ncol)
migration_matrix = np.append(migration_matrix, np.zeros( (1, ncol * nrow) ), axis = 0)
migration_matrix = np.append(migration_matrix, np.zeros(( (ncol * nrow + 1) ,1)), axis = 1)

if demo = "ooa":
  _initial_size = N_EU
  _growth_rate = r_EU
else:
  _initial_size = pop_size
  _growth_rate = 0

if admixture:
  population_configurations = [msprime.PopulationConfiguration(sample_size = 0, initial_size = _initial_size, growth_rate = _growth_rate)] * (ncol * nrow)
  population_configurations.append(msprime.PopulationConfiguration(sample_size = (N + N_ref) * ncol * nrow, initial_size = pop_size))
  demo_events=[msprime.MassMigration(time = time_move, source = i, destination = ncol * nrow, proportion = (ns[i] + ns_ref[i])/(N + N_ref)) for i in range(ncol * nrow)]
else:
  population_configurations = [msprime.PopulationConfiguration(sample_size = n + n_ref, initial_size = _initial_size, growth_rate = _growth_rate) for n, n_ref in zip(ns, ns_ref)]
  population_configurations.append(msprime.PopulationConfiguration(sample_size = 0, initial_size = _initial_size, growth_rate = _growth_rate))
  demo_events=[msprime.MassMigration(time = time_move, source = i, destination = ncol * nrow, proportion = 1.0) for i in range(ncol * nrow)]
  demo_events.append(msprime.MigrationRateChange(time = time_move, rate=0))

if demo == "ooa":
  demo_events += [msprime.PopulationParametersChange(time=T_EU_AS, initial_size = N_B, growth_rate=0),
                  msprime.PopulationParametersChange(time=T_B, initial_size = N_AF, growth_rate=0),
                  msprime.PopulationParametersChange(time=T_AF, initial_size = N_A, growth_rate=0)]
  demo_events.sort(key=lambda x: x.time)

trees = msprime.simulate(population_configurations = population_configurations, migration_matrix = migration_matrix,
                         length = l, recombination_rate = recomb_rate, mutation_rate = mutation_rate, 
                         demographic_events = demo_events)


### compute Fst and then split study and reference
print("computing FST")

study_ids = []
reference_ids = []
start = 0
for i in range(ncol * nrow):
  study_ids += list(range(start, start + ns[i]))
  reference_ids += list(range(start + ns[i], start + ns[i] + ns_ref[i]))
  start += ns[i] + ns_ref[i]
study_ids = np.array(study_ids).astype(int)
reference_ids = np.array(reference_ids).astype(int)

if study_ids.shape[0] >= 20 and reference_ids.shape[0] >= 20:
  fst_direct = Fst2(trees, study_ids, reference_ids)
  fst_subsample = []
  for i in np.arange(100):
    study_ids_ = np.random.choice(study_ids, 20, replace=False)
    reference_ids_ = np.random.choice(reference_ids, 20, replace=False)
    fst_ = Fst2(trees, study_ids_, reference_ids_)
    fst_subsample.append(fst_)
  fst = {"fst_direct":fst_direct, "fst_subsample":np.array(fst_subsample)}
else:
  fst = {}

trees_ref = remove_monomorphic(trees.simplify(reference_ids))
trees = remove_monomorphic(trees.simplify(study_ids))

variants = trees.variants()
MAFs = np.array([v.genotypes.mean() for v in trees.variants()])
MACs = np.array([v.genotypes.sum() for v in trees.variants()])
loci = np.array([v.position for v in trees.variants()])
M = loci.shape[0]


### commons, rares, obss, cass ###
commons = np.where(MAFs >= 0.05)[0]
rares = np.where(np.logical_and(MACs >=2, MACs <= 5))[0]

# observation
idx_5 = MAFs >= 0.005
M_5 = idx_5.sum()
MAFs_5 = MAFs[idx_5]

loci_ceil = np.ceil(loci)
overlapped = np.insert(loci_ceil[:-1] == loci_ceil[1:], 1, False)
non_overlapped = np.logical_not(overlapped)

observable =  np.logical_and(idx_5, non_overlapped)
M_observable = observable.sum()

weights = np.multiply(np.power(MAFs, Beta), np.power(1-MAFs, Beta))
weights = np.multiply(weights, observable)
weights /= weights.sum()

M_obs = int(round(M_observable * obs_ratio))
obss = np.random.choice(np.where(observable)[0], M_obs, replace = False, p = weights[observable]); obss.sort()

# phenotype
M_cas = int(round(M * cas_ratio))
weights = np.multiply(np.power(MAFs, Alpha), np.power(1-MAFs, Alpha))
weights /= weights.sum()
cass = np.random.choice(range(M), M_cas, replace = False, p = weights); cass.sort()
X_cas = getX(trees, cass).astype("float")

betas = np.random.normal(scale = np.sqrt(h2g / M_cas), size = (M_cas, 1))

G = np.dot(X_cas, betas); G.shape = (N, )
s2g = np.var(G, ddof = 1)
s2e = s2g * (1/h2g - 1)

e = np.random.normal(scale=np.sqrt(s2e), size=N)
y = G + e


### compute Ks ###
print("computing Ks")

K_all = getK(trees, np.arange(M))
K_common = getK(trees, commons)
K_rare = getK(trees, rares)
K_obs = getK(trees, obss)
K_cas = getK(trees, cass)

if run_egrm:
  print("computing EK")
  EK, _, EK_mu = varGRM_C(trees, var = False)

if run_mtmrca:
  print("computing mTMRCA")
  mtmrca, _ = mTMRCA_C(trees)


### running impute ###
def X2gen(X):
  n, m = X.shape
  maternals = np.array(range(0, n, 2))
  paternals = np.array(range(1, n, 2))
  X_dip = X[maternals, :] + X[paternals, :]
  
  tmp1 = (X_dip == 2).astype(int)
  tmp2 = (X_dip == 1).astype(int)
  tmp3 = (X_dip == 0).astype(int)
  
  n_gen = int(n * 3 / 2)
  buffer = np.zeros([n_gen, m]).astype(int)
  buffer[np.arange(0, n_gen, 3), :] = tmp1
  buffer[np.arange(1, n_gen, 3), :] = tmp2
  buffer[np.arange(2, n_gen, 3), :] = tmp3
  return buffer

def gen2X(gen):
  n, m = gen.shape
  tmp1 = gen[np.arange(0, n, 3), :]
  tmp2 = gen[np.arange(1, n, 3), :]
  buffer = tmp1 * 2 + tmp2
  return buffer

if run_impute:
  print("computing K_imputed")
  os.mkdir("impute")
  os.chdir("impute")
  
  X_obs = getX(trees, obss)
  gen_obs = X2gen(X_obs)
  loci_obs = np.ceil(loci[obss]).astype(int)
  
  haps_file = open(name + ".gen", 'a')
  i = 0
  for idx, obs in enumerate(obss):
    string = "chr snp" + str(obs+1) + " " + str(loci_obs[idx]) + " A" + " T "
    string = string + " ".join(map(str, gen_obs[:, idx])) + "\n"
    bytes = haps_file.write(string)
    i += 1
  haps_file.close()
  
  X_ref = getX(trees_ref, np.arange(trees_ref.num_mutations))
  gen_ref = X2gen(X_ref)
  loci_ref = np.array([v.position for v in trees_ref.variants()])
  loci_ref = np.ceil(loci_ref).astype(int)
  
  haps_file = open(name + "_ref.gen", 'a')
  i = 0
  for idx, obs in enumerate(np.arange(trees_ref.num_mutations)):
    string = "1 refsnp" + str(obs+1) + " " + str(loci_ref[idx]) + " A" + " T "
    string = string + " ".join(map(str, gen_ref[:, idx])) + "\n"
    bytes = haps_file.write(string)
    i += 1
  haps_file.close()
  
  map_file = open(name + ".map",'a')
  map_file.write("pos COMBINED_rate Genetic_Map\n")
  for idx, obs in enumerate(obss):
    string = str(loci_obs[idx]) + " " + str(1) + " "
    string = string + str(loci_obs[idx]/1000000) + "\n"
    bytes = map_file.write(string)
  map_file.close()
  
  os.system("~/bin/impute2 " + \
            "-g_ref " + name + "_ref.gen " + \
            "-m " + name + ".map " + \
            "-g " + name + ".gen " + \
            "-int 0 " + str(l) + " -allow_large_regions " + \
            "-o out.gen")
  
  gen_imputed = pd.read_table('out.gen', sep = ' ', header = None).iloc[:, 5:]
  gen_imputed = np.transpose(gen_imputed.values)
  X_imputed = gen2X(gen_imputed)
  
  keep = np.logical_and(X_imputed.mean(axis = 0) > 0, X_imputed.mean(axis = 0) < 1)
  X_imputed = X_imputed[:, keep]
  
  Z_imputed = X_imputed
  Z_imputed -= Z_imputed.mean(axis=0); Z_imputed /= Z_imputed.std(axis=0)
  K_imputed = np.dot(Z_imputed, np.transpose(Z_imputed))
  
  os.chdir("..")


### running relate ###
if run_relate:
  print("computing EK_relate")
  os.mkdir("relate")
  os.chdir("relate")
  
  X_obs = getX(trees, obss)
  loci_obs = np.ceil(loci[obss]).astype(int)
  
  haps_file = open(name + ".haps", 'a')
  i = 0
  for idx, obs in enumerate(obss):
    string = "1 snp" + str(obs+1) + " " + str(loci_obs[idx]) + " A" + " T "
    string = string + " ".join(map(str, X_obs[:, idx])) + "\n"
    bytes = haps_file.write(string)
    i += 1
  haps_file.close()
  os.system("gzip -f " + name + ".haps > " + name + ".haps.gz && rm -f " + name + ".haps")
  
  sample_file = open(name + ".sample",'a')
  sample_file.write("ID_1 ID_2 missing\n0 0 0\n")
  for idx in range(int(N)):
    string = "UNR" + str(idx+1) + " NA" + " 0\n"
    bytes = sample_file.write(string)
  sample_file.close()
  
  map_file = open(name + ".relate.map",'a')
  map_file.write("pos COMBINED_rate Genetic_Map\n")
  for idx, obs in enumerate(obss):
    string = str(loci_obs[idx]) + " " + str(1) + " "
    string = string + str(loci_obs[idx]/1000000) + "\n"
    bytes = map_file.write(string)
  map_file.close()
  
  os.system("~/bin/relate.sh " + name + ".haps.gz " + name + ".sample " + name + ".relate.map " + name)
  
  trees_relate = tskit.load(name + ".trees")
  EK_relate, _, EK_relate_mu = varGRM_C(trees_relate)
  
  os.chdir("..")


### running tsinfer
if run_tsinfer:
  print("computing EK_tsinfer")
  os.mkdir("tsinfer")
  os.chdir("tsinfer")
  
  X_obs = getX(trees, obss)
  loci_obs = np.ceil(loci[obss]).astype(int)
  with tsinfer.SampleData(path = name + ".samples", sequence_length = l) as sample_data:
    for idx, obs in enumerate(obss):
      sample_data.add_site(loci_obs[idx], X_obs[:, idx])
  
  os.system("~/bin/tsinfer infer " + name + ".samples -p -t 4")
  
  trees_tsinfer = tskit.load(name + ".trees")
  trees_tsdate = tsdate.date(trees_tsinfer, Ne = 10000)
  
  EK_tsinfer, _, EK_tsinfer_mu = varGRM_C(trees_tsinfer)
  EK_tsdate, _, EK_tsdate_mu = varGRM_C(trees_tsdate)
  
  os.chdir("..")


### collect Ks and compute correlation ###
Ks = {"K_all":K_all, "K_common":K_common, "K_rare":K_rare, "K_obs":K_obs, "K_cas":K_cas}
if run_egrm:
  Ks['EK'] = EK
if run_relate:
  Ks["EK_relate"] = EK_relate
if run_tsinfer:
  Ks["EK_tsinfer"] = EK_tsinfer
  Ks["EK_tsdate"] = EK_tsdate
if run_mtmrca:
  Ks["mtmrca"] = mtmrca

# haploid
diags = np.diag_indices(int(N))
non_diags = np.where(~np.eye(int(N),dtype=bool))
table = {}
for key in Ks.keys():
  table[key] = Ks[key][non_diags].flatten()

table = pd.DataFrame(data=table)
corr = table.corr(method ='pearson')
corr_spearman = table.corr(method ='spearman')

# diploid
maternals = np.array(range(0, N, 2))
paternals = np.array(range(1, N, 2))
Ks_dip = {}
for key in Ks.keys():
  Ks_dip[key] = 0.5 * (Ks[key][maternals, :][:, maternals] + Ks[key][maternals, :][:, paternals] + \
                       Ks[key][paternals, :][:, maternals] + Ks[key][paternals, :][:, paternals])

if run_impute:
  Ks_dip['K_imputed'] = K_imputed

diags = np.diag_indices(int(N/2))
non_diags = np.where(~np.eye(int(N/2),dtype=bool))
table_dip = {}
for key in Ks_dip.keys():
  table_dip[key] = Ks_dip[key][non_diags].flatten()
  
table_dip = pd.DataFrame(data=table_dip)
corr_dip = table_dip.corr(method ='pearson')


### PCA and SI ###
print("computing PCA and SI")

PCA = {}
for key in Ks_dip.keys():
  K = Ks_dip[key]
  w, v = np.linalg.eig(K)
  idx = w.argsort()[::-1]
  w = w[idx]; v = v[:, idx]
  ndim = min(10, (w >= 0).sum())
  pc = np.dot(v[:, :ndim], np.diag(np.power(w[:ndim], 0.5)))
  PCA[key] = pc

true_labels = np.repeat(np.arange(len(ns)), (np.array(ns)/2).astype(int))
SI = {}
for key in PCA.keys():
  SI[key] = np.array([separation_index(true_labels, PCA[key][:, :i]).mean() for i in range(1, 11)])
SI = pd.DataFrame(SI)

### output ###
results = {"args":args, "fst":fst, "obss":obss, "Ks":Ks, "Ks_dip":Ks_dip, "corr":corr, "corr_spearman":corr_spearman, "corr_dip":corr_dip, "PCA":PCA, "SI":SI}

with open("results.p", 'wb') as f:
  pickle.dump(results, f)

trees.dump("simulation.trees")
